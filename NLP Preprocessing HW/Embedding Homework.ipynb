{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMrYpafqA43+LlPNfPbs+RB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Assignment Week 10**\n","\n","Due: 4/1/2024\n","\n","Shimon Greengart\n","\n","First, I get the data using the professor's code from last lecture."],"metadata":{"id":"dzT0qAVClFgi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BAOJl70JiJib","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711667004639,"user_tz":240,"elapsed":25774,"user":{"displayName":"Shimon Greengart","userId":"09586836330961698645"}},"outputId":"90623a9d-c2dc-4713-9f30-22fd7362cbb5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n","Collecting kaggle\n","  Downloading kaggle-1.6.8.tar.gz (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.6/84.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n","Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n","Building wheels for collected packages: kaggle\n","  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kaggle: filename=kaggle-1.6.8-py3-none-any.whl size=111962 sha256=1ca1bf77bc1515a5fd6a791f8c1389851a7788f2e422f14f81b832e5b6c6da4e\n","  Stored in directory: /root/.cache/pip/wheels/b3/86/2d/2df535a84838c858ca91a03805f9c3131573c8e777fd907689\n","Successfully built kaggle\n","Installing collected packages: kaggle\n","  Attempting uninstall: kaggle\n","    Found existing installation: kaggle 1.5.16\n","    Uninstalling kaggle-1.5.16:\n","      Successfully uninstalled kaggle-1.5.16\n","Successfully installed kaggle-1.6.8\n","writing kaggle\n","got kaggle json copied\n","Kaggle API 1.6.8\n","Downloading sentiment-analysis-on-movie-reviews.zip to /content\n","  0% 0.00/1.90M [00:00<?, ?B/s]\n","100% 1.90M/1.90M [00:00<00:00, 74.2MB/s]\n","Archive:  sentiment-analysis-on-movie-reviews.zip\n","  inflating: sampleSubmission.csv    \n","  inflating: test.tsv.zip            \n","  inflating: train.tsv.zip           \n"]}],"source":["import pandas as pd\n","\n","!pip install kaggle --upgrade\n","print (\"writing kaggle\")\n","!echo \"{\\\"username\\\":\\\"shimongreengart\\\",\\\"key\\\":\\\"c659755ac5876b422e520ad3530ab5f5\\\"}\" > kaggle.json\n","!sudo mkdir -p ~/.kaggle\n","!sudo cp /content/kaggle.json ~/.kaggle/kaggle.json\n","print (\"got kaggle json copied\")\n","!chmod 600 /root/.kaggle/kaggle.json\n","!kaggle --version\n","!kaggle competitions download -c sentiment-analysis-on-movie-reviews\n","\n","!unzip sentiment-analysis-on-movie-reviews.zip\n","\n","train_df = pd.read_csv('/content/train.tsv.zip', compression='zip', sep='\\t')"]},{"cell_type":"code","source":["print(train_df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eXFvr8xpj9sO","executionInfo":{"status":"ok","timestamp":1711670532165,"user_tz":240,"elapsed":155,"user":{"displayName":"Shimon Greengart","userId":"09586836330961698645"}},"outputId":"714f04bb-8ec5-42e0-adea-f6030fb6459d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   PhraseId  SentenceId                                             Phrase  \\\n","0         1           1  A series of escapades demonstrating the adage ...   \n","1         2           1  A series of escapades demonstrating the adage ...   \n","2         3           1                                           A series   \n","3         4           1                                                  A   \n","4         5           1                                             series   \n","\n","   Sentiment  \n","0          1  \n","1          2  \n","2          2  \n","3          2  \n","4          2  \n"]}]},{"cell_type":"markdown","source":["I heavily used this link when figuring out what to do: https://www.geeksforgeeks.org/nlp-gensim-tutorial-complete-guide-for-beginners/.\n","\n","First, I took the phrases given and tokenized them. I used gensim's simple_preprocess function to lowercase everything and remove 1-letter words. Then I used the list of tokenized phrases to train a Word2Vec model using gensim's Word2Vec creator."],"metadata":{"id":"y52XWP5imvke"}},{"cell_type":"code","source":["import gensim as gs\n","import numpy as np\n","import os\n","\n","train_np = train_df.to_numpy()\n","\n","all_phrases = train_np[:, 2]\n","\n","tokenized = []\n","for phrase in all_phrases:\n","    tokenized.append(gs.utils.simple_preprocess(phrase))\n","\n","# Normally, you would use a prebuilt word2vec or the like for your embedding\n","# But you wanted us to train a new one\n","w2v_model = gs.models.word2vec.Word2Vec(sentences=tokenized, workers=os.cpu_count())\n","# If I want, I could have a min_count ignoring rare words\n","# But I don't know what threshold I would use\n","# And given that the dataset contains many subsets of each review, I'm not even sure if it would work\n","\n","# word vector for the word \"time\", to get visual indication\n","print(w2v_model.wv['time'])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WzMwGyfOoj4X","executionInfo":{"status":"ok","timestamp":1711667021600,"user_tz":240,"elapsed":16965,"user":{"displayName":"Shimon Greengart","userId":"09586836330961698645"}},"outputId":"ca16dd1b-c78b-40b2-dfb4-9674fc205aa5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[-0.58479345  1.8454291   1.3871014   0.20396076  0.11060505 -1.1130906\n","  1.1975371   0.50944686 -0.26329017  1.5033654   0.5495204  -0.7427096\n","  2.3862298   0.89145994 -0.44224218  1.5086163   1.4250164  -0.78379273\n"," -1.4392302  -0.85365295  0.55524343 -1.160308    1.0822506  -1.7084757\n"," -2.028357   -0.5883192  -1.8655173   1.9735564  -1.9622779   1.0758284\n"," -1.1275796   0.79752004  2.3454978  -0.14222625 -0.41791037  0.02216911\n","  0.29475316 -0.5183934  -2.128953   -0.61547214 -0.15247567  2.1783714\n"," -1.6378458   0.09207394  0.00904999  0.6541906   0.60721916 -0.13557452\n"," -1.3757231   1.6014705  -0.35181737 -1.2583125  -2.3208618  -0.37793788\n","  1.4651257   0.37592816  0.6355673   1.860414   -2.0122056   1.256376\n"," -2.2940717  -1.353127    2.0184145   2.3103702  -0.5965205   0.43871164\n"," -0.06816463  0.15809098 -0.19589262  0.40652815 -0.43776125 -3.0955908\n","  0.10209551  2.0006943   0.99855435  1.0352426   1.2796441  -0.5904884\n"," -1.9830823   0.78886884  1.999134    0.66697234 -1.3769152   0.9516883\n"," -1.7708184   0.5293046   3.2795808   0.13101764 -0.8164293  -1.7875375\n","  1.4738032   0.2420353  -2.173702    2.0863204  -0.54768896  0.7996514\n"," -3.943908    1.4853208   0.6453738  -1.1199362 ]\n"]}]},{"cell_type":"markdown","source":["Next I check similar words. You said we could use whatever movie-related words we wanted, so I chose \"action\", \"comedy\", \"horror\", \"adventure\", and \"popcorn\". The first four are genres and the last one is fun.\n","\n","It was interesting. The word most similar to comedy is \"quadrangle\" (which I'm not even sure how it got into the dataset). The word most similar to horror is \"mediocre\". Ouch. And the word most similar to \"popcorn\" is \"phony\", probably due to complaining about phony popcorn. \"Horror\" was the 7th-closest word to \"action\", but not vice versa. Likewise, \"comedies\" is the 2nd-closest word to \"comedy\" and the 11th-closest word to \"adventure\". Anf for some reason, \"popcorn\" is the 8th-most similar word to \"horror\", though not vice-versa. And \"thriller\" was apparently much closer to \"comedy\" than \"adventure\" or \"action\" for reasons I can't determine.\n","\n","One thing I noticed is that most of these words weren't arbitrary. I suspect that if I spent more time looking at the reviews, the weirder ones would make more sense. I am wondering if some of horror's stranger associations are because sometimes, \"horror\" refers to the genre, while other times, it refers to how terrible a movie is."],"metadata":{"id":"fU4XeNPDTaVi"}},{"cell_type":"code","source":["for word in [\"action\", \"comedy\", \"horror\", \"adventure\", \"popcorn\"]:\n","    print(f\"Most similar to {word}:\")\n","    raw_duples = w2v_model.wv.most_similar(word, topn=12)\n","    print([duple[0] for duple in raw_duples])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ilFDXx1TTYuc","executionInfo":{"status":"ok","timestamp":1711667224215,"user_tz":240,"elapsed":213,"user":{"displayName":"Shimon Greengart","userId":"09586836330961698645"}},"outputId":"fea6146d-0880-4171-fd71-af73732f2ac5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Most similar to action:\n","['animated', 'gangster', 'hong', 'monster', 'kong', 'spy', 'horror', 'abrasive', 'kiddie', 'goofball', 'buddy', 'flick']\n","Most similar to comedy:\n","['quadrangle', 'comedies', 'incurably', 'trifle', 'thriller', 'dreaming', 'drama', 'volatile', 'hybrid', 'classic', 'fare', 'psychedelia']\n","Most similar to horror:\n","['mediocre', 'monster', 'franchise', 'recent', 'rare', 'worst', 'creepiest', 'popcorn', 'summer', 'baseball', 'thornberrys', 'event']\n","Most similar to adventure:\n","['modernized', 'mib', 'fare', 'formula', 'experience', 'suitable', 'year', 'extreme', 'musketeer', 'recycles', 'comedies', 'weepy']\n","Most similar to popcorn:\n","['phony', 'solidly', 'jumbled', 'stones', 'evenings', 'baloney', 'types', 'chan', 'slapdash', 'scum', 'overstating', 'marvelous']\n"]}]},{"cell_type":"markdown","source":["Here, I experimented with word subtraction, and these are results I found interesting (or at least humorous). I found that most of the time, if I try (x - y), the result is x, presumably because the subtractor vector wasn't big enough to significantly change the subtractand vector. But I'm skeptical that there is really all that much meaningful here. While it is true that a child without their mother is immediate suspense (the second option, funnily enough, was school), I found enough meaningless differences to make me skeptical that any of them *really* mean anything.\n","\n","I only used subtraction, not addition, because I found that with addition, the vectors were too big to get any new values. Perhaps with a bigger corpus that uses words in more ways, this would work better."],"metadata":{"id":"SpQ_uliBekis"}},{"cell_type":"code","source":["word_pairs = [('majestic', 'king'), ('movie', 'story'), ('funny', 'comedy'), ('franchise', 'action'), ('cat', 'person'), ('child', 'mother'), ('war', 'man'), ('man', 'old'), ('woman', 'young')]\n","\n","for duple in word_pairs:\n","    diff_vec = w2v_model.wv[duple[0]] - w2v_model.wv[duple[1]]\n","    result = w2v_model.wv.most_similar(diff_vec, topn=1)[0][0]\n","    print(f\"{duple[0]} - {duple[1]} = {result}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"givu7RcUVc2o","executionInfo":{"status":"ok","timestamp":1711670105724,"user_tz":240,"elapsed":143,"user":{"displayName":"Shimon Greengart","userId":"09586836330961698645"}},"outputId":"211b07bb-e780-4dab-c05b-f757de4a6b4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["majestic - king = impair\n","movie - story = olds\n","funny - comedy = elizabethans\n","franchise - action = appreciative\n","cat - person = mothers\n","child - mother = suspense\n","war - man = mystery\n","man - old = character\n","woman - young = overrides\n"]}]},{"cell_type":"code","source":["calculations = (w2v_model.wv[duple[0]] - w2v_model.wv[duple[1]] for duple in word_pairs)\n","for vector in calculations:\n","    raw_duples = w2v_model.wv.most_similar(vector, topn=5)\n","    print([duple[0] for duple in raw_duples])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wnJnuknSfSeV","executionInfo":{"status":"ok","timestamp":1711670113490,"user_tz":240,"elapsed":147,"user":{"displayName":"Shimon Greengart","userId":"09586836330961698645"}},"outputId":"70db37b8-c77d-49ef-c45f-c2fa800209a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['impair', 'enactments', 'illuminates', 'clinch', 'it']\n","['olds', 'summer', 'fans', 'wrestling', 'sci']\n","['elizabethans', 'funny', 'lamentations', 'answers', 'simply']\n","['appreciative', 'proving', 'dupe', 'swaying', 'bills']\n","['mothers', 'political', 'tension', 'masturbation', 'violence']\n","['suspense', 'school', 'crumb', 'urgency', 'center']\n","['mystery', 'generated', 'fiction', 'processed', 'courtroom']\n","['character', 'story', 'being', 'man', 'touch']\n","['overrides', 'says', 'thrilled', 'dungpile', 're']\n"]}]},{"cell_type":"markdown","source":["I was wondering if my model was suffering because their dataset was including many subphrases, so I tried training another version only with the full sentences. But I found that it was actually much worse, from the perspective that the words most similar to my 5 sample words seem completely irrelevant. At least in the first version, they were mostly nouns. Here, the first 3 entries for \"action\" are prepositions. So, it looks like more data, even repetitive data, was helpful."],"metadata":{"id":"G6-dRjPvgOev"}},{"cell_type":"code","source":["id_set = set()\n","sentences = []\n","for row in train_np:\n","    if row[1] not in id_set:\n","        id_set.add(row[1])\n","        sentences.append(row[2])\n","\n","print(len(sentences))\n","\n","sent_tokenized = []\n","for sentence in sentences:\n","  # the simple_preprocess function returns a list of each sentence\n","    sent_tokenized.append(gs.utils.simple_preprocess(sentence))\n","\n","# Normally, you would use a prebuilt word2vec or the like for your embedding\n","# But you wanted us to train a new one\n","sent_model = gs.models.word2vec.Word2Vec(sentences=sent_tokenized, workers=os.cpu_count())\n","\n","for word in [\"action\", \"comedy\", \"horror\", \"adventure\", \"popcorn\"]:\n","    print(f\"Most similar to {word}:\")\n","    raw_duples = sent_model.wv.most_similar(word, topn=12)\n","    print([duple[0] for duple in raw_duples])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NB7xi8sJpe6Q","executionInfo":{"status":"ok","timestamp":1711670331123,"user_tz":240,"elapsed":1140,"user":{"displayName":"Shimon Greengart","userId":"09586836330961698645"}},"outputId":"10a7c67e-3a14-47f2-f07d-8d1942a3bdac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["8529\n","Most similar to action:\n","['between', 'into', 'while', 'dialogue', 'american', 'tale', 'without', 'some', 'comic', 'performances', 'plot', 'three']\n","Most similar to comedy:\n","['both', 'some', 'heart', 'full', 'dialogue', 'emotional', 'whose', 'old', 'style', 'despite', 'new', 'drama']\n","Most similar to horror:\n","['full', 'mr', 'special', 'off', 'picture', 'being', 'through', 'ultimately', 'comic', 'first', 'tone', 'takes']\n","Most similar to adventure:\n","['piece', 'without', 'american', 'yet', 'acting', 'off', 'takes', 'thriller', 'dialogue', 'often', 'human', 'comic']\n","Most similar to popcorn:\n","['nearly', 'every', 'series', 'through', 'music', 'great', 'fun', 'being', 'making', 'me', 'our', 'movies']\n"]}]}]}